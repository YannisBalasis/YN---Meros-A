import pandas as pd
import re
import nltk
import sys
import codecs
import numpy as np
import matplotlib.pyplot as plt
from collections import Counter
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import GridSearchCV




sys.stdout = codecs.getwriter("utf-8")(sys.stdout.detach())

# Διάβασμα του αρχείου CSV
dataset = pd.read_csv("C:/Python/Computational Intelligence/Exercise1/iphi2802.csv", header=0, sep='\t', encoding='utf-8')

# Ορισμός του tokenizer
def tokenize_text(text):
    words = word_tokenize(text)  # Tokenization
    stop_words = set(stopwords.words('english'))
    filtered_words = [word for word in words if word.lower() not in stop_words]  # Αφαίρεση των στοπ λέξεων
    words = [word for word in filtered_words if re.match(r'\b\w+\b', word)]  # Αφαίρεση των χαρακτήρων
    return words

# Εφαρμογή του tokenizer στη στήλη 'text'
dataset['tokenized_text'] = dataset['text'].apply(tokenize_text)

# Ενώση  λέξεων σε ένα μεγάλο κείμενο
all_words = [word for sublist in dataset['tokenized_text'] for word in sublist]

# Υπολογισμός συχνότητας των λέξεων
word_freq = Counter(all_words)

# Επιλογή των 1000 πιο συχνών λέξεων
most_common_words = word_freq.most_common(1000)
print(most_common_words)

# Συνένωση των λέξεων σε ένα κείμενο
clean_text = ' '.join(all_words)

# Χρήση του TfidfVectorizer για το tf-idf
vectorizer = TfidfVectorizer(max_features=1000)
X = vectorizer.fit_transform([clean_text])

# Εκτύπωση του tf-idf
print(X)

# Δημιουργία ενός MixMaxScaler αντικειμένου 
scaler = MinMaxScaler(feature_range=(-1,1))

# Ορισμός των στηλών που θέλω να γίνει η κανονικοποίηση (date_min - date_max)
to_normalize = ['date_min', 'date_max']

# Κανονικοποίηση
dataset[to_normalize] = scaler.fit_transform(dataset[to_normalize])

# Εκτύπωση αποτελέσματος
#print(dataset[['date_min', 'date_max']].to_string(index=False))

# Διαχωρισμός των δεδομένων σε χαρακτηριστικά (X) και ετικέτες (y)
X = dataset['text']  
y_min = dataset['date_min']  
y_max = dataset['date_max'] 

# Διαχωρισμός των δεδομένων σε σύνολα εκπαίδευσης και ελέγχου
X_train, X_test, y_min_train, y_min_test, y_max_train, y_max_test = train_test_split(X, y_min, y_max, test_size=0.2, random_state=42)

# Μετατροπή του κειμένου σε TF-IDF χαρακτηριστικά
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

model_min = MLPRegressor(hidden_layer_sizes=(50,),max_iter=500, activation='relu', solver='adam', learning_rate_init=0.001, random_state=42,early_stopping= True)
model_max = MLPRegressor(hidden_layer_sizes=(50,),max_iter=500 ,activation='relu', solver='adam', learning_rate_init=0.001, random_state=42,early_stopping= True)

# Ορισμός του MLPRegressor μοντέλου για το επίπεδο εξόδου με γραμμική συνάρτηση
model_min_output = MLPRegressor(hidden_layer_sizes=(),max_iter=500 , activation='identity', solver='adam', learning_rate_init=0.001, random_state=42,early_stopping= True)
model_max_output = MLPRegressor(hidden_layer_sizes=(),max_iter = 500, activation='identity', solver='adam', learning_rate_init=0.001, random_state=42,early_stopping= True)
# Εκπαίδευση των μοντέλων για το επίπεδο εξόδου

# Διασταυρούμενη επικύρωση με 5 folds
k_fold = KFold(n_splits=5, shuffle=True, random_state=42)

# Εφαρμογή της διασταυρούμενης επικύρωσης
for train_indices, val_indices in k_fold.split(X_train_tfidf):
    X_train_fold, X_val_fold = X_train_tfidf[train_indices], X_train_tfidf[val_indices]
    y_min_train_fold, y_min_val_fold = y_min_train.iloc[train_indices], y_min_train.iloc[val_indices]
    y_max_train_fold, y_max_val_fold = y_max_train.iloc[train_indices], y_max_train.iloc[val_indices]
    
    # Εκπαίδευση του  για την ελάχιστη ημερομηνία
    model_min.fit(X_train_fold, y_min_train_fold)
    
    # Εκπαίδευση του  για τη μέγιστη ημερομηνία
    model_max.fit(X_train_fold, y_max_train_fold)
    
    # Αξιολόγηση των μοντέλων στα δεδομένα επικύρωσης
    predictions_min = model_min.predict(X_val_fold)
    mse_min = mean_squared_error(y_min_val_fold, predictions_min)
    
    predictions_max = model_max.predict(X_val_fold)
    mse_max = mean_squared_error(y_max_val_fold, predictions_max)
    

# Αξιολόγηση των μοντέλων στα δεδομένα ελέγχου
test_predictions_min = model_min.predict(X_test_tfidf)
test_mse_min = mean_squared_error(y_min_test, test_predictions_min)
print("Test Min Mean Squared Error:", test_mse_min)

test_predictions_max = model_max.predict(X_test_tfidf)
test_mse_max = mean_squared_error(y_max_test, test_predictions_max)
print("Test Max Mean Squared Error:", test_mse_max)
    
# ερωτημα Α2 Α
def calculate_rmse(predictions, y_min, y_max):
    errors = []
    min_length = min(len(predictions), len(y_min), len(y_max))
    for i in range(min_length):
        if predictions[i] < y_min.iloc[i]:
            errors.append(y_min.iloc[i] - predictions[i])
        elif predictions[i] > y_max.iloc[i]:
            errors.append(predictions[i] - y_max.iloc[i])
        else:
            errors.append(0)
    rmse = np.sqrt(np.mean(np.square(errors)))
    return rmse

# Αξιολόγηση στα δεδομένα εκπαίδευσης
test_rmse_min = calculate_rmse(test_predictions_min, y_min_test, y_max_test)
test_rmse_max = calculate_rmse(test_predictions_max, y_min_test, y_max_test)

print("Test RMSE (Min):", test_rmse_min)
print("Test RMSE (Max):", test_rmse_max)

#ερωτημα δ 
# Ορισμός των διαφορετικών αριθμών νευρώνων
hidden_layer_sizes = [50, 100, 150]

# Εκπαίδευση των μοντέλων και δημιουργία γραφημάτων σύγκλισης για κάθε μοντέλο
for size in hidden_layer_sizes:
   
    model = MLPRegressor(hidden_layer_sizes=(size,), max_iter=500, activation='relu', solver='adam', learning_rate_init=0.001, random_state=42,early_stopping= True)

    #  KFold
    k_fold = KFold(n_splits=5, shuffle=True, random_state=42)

    rmse_values = []
    loss_curves = []

    for train_indices, val_indices in k_fold.split(X_train_tfidf):
        X_train_fold, X_val_fold = X_train_tfidf[train_indices], X_train_tfidf[val_indices]
        y_min_train_fold, y_min_val_fold = y_min_train.iloc[train_indices], y_min_train.iloc[val_indices]
        
        # Εκπαίδευση 
        model.fit(X_train_fold, y_min_train_fold)
        
        # Αποθήκευση της καμπύλης απώλειας
        loss_curves.append(model.loss_curve_)

         # Πρόβλεψη για τα δεδομένα ελέγχου
        y_pred = model.predict(X_val_fold)
        
        # Υπολογισμός του RMSE
        rmse = np.sqrt(mean_squared_error(y_min_val_fold, y_pred))
        
        # Αποθήκευση του RMSE
        rmse_values.append(rmse)

    # Σχεδίαση  για κάθε fold 
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.set_title(f'Convergence Plot for Hidden Layer Size = {size}')
    ax.set_xlabel('Epochs')
    ax.set_ylabel('Training Loss')

    # Σχεδίαση των καμπυλών απώλειας για κάθε fold
    for i, curve in enumerate(loss_curves):
        ax.plot(range(1, len(curve) + 1), curve, label=f'Fold={i+1}')

    ax.legend()
    plt.show()

       # Εκτύπωση του RMSE για κάθε fold
    print(f'RMSE for Hidden Layer Size = {size}: {rmse_values}')

 
print("------------------------------------------")

#ερωτημα ε  α2 
# Ορίζουμε τους διάφορους συνδυασμούς του αριθμού των κρυφών επιπέδων
hidden_layer_sizes_list_DIFF = [(50, 100), (50, 150), (100, 150), (50, 100, 150)]
hidden_layer_sizes_list_SAME = [(50, 50), (50, 50, 50)]
hidden_layer_sizes_lists = [hidden_layer_sizes_list_DIFF, hidden_layer_sizes_list_SAME]

for i, hidden_layer_sizes_list in enumerate(hidden_layer_sizes_lists):
    for hidden_layer_sizes in hidden_layer_sizes_list:
        k_fold = KFold(n_splits=5, shuffle=True, random_state=42)
        rmse_values = []
        for train_indices, val_indices in k_fold.split(X_train_tfidf):
            X_train_fold, X_val_fold = X_train_tfidf[train_indices], X_train_tfidf[val_indices]
            y_min_train_fold, y_min_val_fold = y_min_train.iloc[train_indices], y_min_train.iloc[val_indices]
            
            # Δημιουργία και εκπαίδευση του μοντέλου
            model = MLPRegressor(hidden_layer_sizes=hidden_layer_sizes, max_iter=500, activation='relu', solver='adam', learning_rate_init=0.001, random_state=42,early_stopping= True)
            model.fit(X_train_fold, y_min_train_fold)
            
            # Πρόβλεψη και υπολογισμός του RMSE
            y_pred = model.predict(X_val_fold)
            rmse = np.sqrt(mean_squared_error(y_min_val_fold, y_pred))
            
            # Αποθήκευση του RMSE
            rmse_values.append(rmse)
        
        # Υπολογισμός του μέσου όρου του RMSE
        avg_rmse = np.mean(rmse_values)
        if i == 0:
            print(f'Average RMSE for Different Hidden Layer Sizes: {hidden_layer_sizes} - {avg_rmse}')
        else:
            print(f'Average RMSE for Same Hidden Layer Sizes: {hidden_layer_sizes} - {avg_rmse}')


#ερωτημα Α3

# Ορίζουμε τα ζευγάρια υπερπαραμέτρων που θέλουμε να εξετάσουμε
pairs = [
    (0.001, 0.2),
    (0.001, 0.6),
    (0.05, 0.6),
    (0.1, 0.6)
]

# Ορίζουμε το μοντέλο MLPRegressor
model = MLPRegressor(hidden_layer_sizes=(50, 50), max_iter=500, activation='relu', solver='adam', learning_rate_init=0.001, random_state=42, early_stopping=True)

# Αρχικοποιούμε λίστες για τα RMSE και τις καμπύλες απώλειας
rmse_values = []
loss_curves = []

# Εφαρμόζουμε GridSearch για κάθε ζευγάρι υπερπαραμέτρων
for eta, momentum in pairs:
    # Δημιουργούμε το πλέγμα υπερπαραμέτρων για το συγκεκριμένο ζευγάρι
    param_grid = {'learning_rate_init': [eta], 'momentum': [momentum]}
    
    # Εφαρμόζουμε GridSearchCV
    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5, verbose=2)
    grid_search.fit(X_train_tfidf, y_min_train)
    
    # Εκπαιδεύουμε το μοντέλο με τις βέλτιστες παραμέτρους
    best_model = grid_search.best_estimator_
    best_model.fit(X_train_tfidf, y_min_train)
    
    # Υπολογισμός του RMSE για τα δεδομένα ελέγχου
    test_predictions = best_model.predict(X_test_tfidf)
    test_mse = mean_squared_error(y_min_test, test_predictions)
    test_rmse = np.sqrt(test_mse)
    rmse_values.append(test_rmse)
    
    # Αποθηκεύουμε την καμπύλη απώλειας
    loss_curves.append(best_model.loss_curve_)

    # Σχεδίαση της καμπύλης απώλειας για το συγκεκριμένο ζευγάρι
    plt.plot(best_model.loss_curve_, label=f'η={eta}, m={momentum}')

# Εμφάνιση της καμπύλης απώλειας
plt.xlabel('Epochs')
plt.ylabel('Training Loss')
plt.title('Loss Curve for Different Hyperparameter Combinations')
plt.legend()
plt.show()

# Εκτύπωση των RMSE για κάθε ζευγάρι υπερπαραμέτρων
for i, (eta, momentum) in enumerate(pairs):
    print(f'RMSE for η={eta}, m={momentum}: {rmse_values[i]}')


#Α4 DROPOUT
# Ορίζουμε τις πιθανότητες διατήρησης rin και rh
rin_values = [0.8, 0.5]
rh_values = [0.5, 0.2]

for rin in rin_values:
    for rh in rh_values:
        # Δημιουργία και εκπαίδευση του μοντέλου με το συγκεκριμένο rin και rh
        model = MLPRegressor(hidden_layer_sizes=(50,50), max_iter=500, activation='relu', solver='adam', learning_rate_init=0.001, random_state=42, early_stopping=True)
        
        # Προσθήκη Dropout layer με πιθανότητες διατήρησης rin και rh
        model = MLPRegressor(hidden_layer_sizes=(50,50), max_iter=500, activation='relu', solver='adam', learning_rate_init=0.001, random_state=42, early_stopping=True)
        
        # Διασταυρούμενη επικύρωση με 5 folds
        k_fold = KFold(n_splits=5, shuffle=True, random_state=42)
        
        rmse_values = []
        loss_curves = []  # Ανανέωση της μεταβλητής loss_curves

        for train_indices, val_indices in k_fold.split(X_train_tfidf):
            X_train_fold, X_val_fold = X_train_tfidf[train_indices], X_train_tfidf[val_indices]
            y_min_train_fold, y_min_val_fold = y_min_train.iloc[train_indices], y_min_train.iloc[val_indices]
            y_max_train_fold, y_max_val_fold = y_max_train.iloc[train_indices], y_max_train.iloc[val_indices]

            # Εκπαίδευση του μοντέλου
            model.fit(X_train_fold, y_min_train_fold)
            
            # Αξιολόγηση του μοντέλου 
            predictions = model.predict(X_val_fold)
            rmse = calculate_rmse(predictions, y_min_val_fold, y_max_val_fold)
            rmse_values.append(rmse)
            loss_curves.append(model.loss_curve_)  # Αποθήκευση της καμπύλης απώλειας
        
        # Υπολογισμός του  RMSE για το συγκεκριμένο ζεύγος rin και rh
        avg_rmse = np.mean(rmse_values)
        print(f'RMSE for rin={rin}, rh={rh}: {avg_rmse}')

        # Εκτύπωση των RMSE για κάθε fold
        print(f'RMSE values for each fold: {rmse_values}')
        print('------------------------------------------')
         
        # Σχεδίαση της καμπύλης απώλειας
        plt.figure(figsize=(10, 6))
        plt.title(f'Loss Curve for rin={rin}, rh={rh}')
        plt.xlabel('Epochs')
        plt.ylabel('Training Loss')

        # Σχεδίαση
        for i, curve in enumerate(loss_curves):
            plt.plot(range(1, len(curve) + 1), curve, label=f'Fold={i+1}')

        plt.legend()
        plt.show()
